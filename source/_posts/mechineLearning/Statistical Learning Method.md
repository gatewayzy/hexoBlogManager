
---
title: Statistical Learning Method
comments: true
date: 2017-01-14 13:58:19
updated: 2017-01-14 13:58:19
categories: Mechine Learning
tags:
- 统计学习方法
---

**说明：**统计学习方法，包括统计学习方法概论、感知机、kNN、SVM等等。
<!-- more -->

---
* 参考文章：
	* 《统计学习方法》


## 统计学习方法

---
### 概论

* 统计学习
	* 监督学习
	* 非监督学习
	* 半监督学习
	* 强化学习
* 输入空间、特征空间、输出空间
* 主要的预测问题
	* 回归问题：输入与输出均为连续变量
	* 分类问题：输出为离散
	* 标注问题：输入与输出都是变量序列
* 统计学习三要素
	* 方法=模型+策略+算法
	* 模型：概率模型（条件概率）、非概率模型（如决策函数）
	* 策略：
		* 损失函数（代价函数），模型预测与真实值的不一致。常用的0-1损失函数、平方损失、绝对损失、对数损失（对数似然损失等）
		* 风险函数（期望损失），模型在联合分布下的期望损失
		* 经验风险最小化，模型在训练集上的平均损失最小化，数据量小时易出现过拟合
		* 结构风险最小化，在经验风险上添加一个正则化项（罚项）进行正则化，让结构风险最小
	* 算法：如何求全局最优等算法
* 模型评估与选择
	* 训练误差与测试误差
	* 过拟合overfit与模型选择，过拟合是对已知数据很符合但对未知数据不适用，缺乏泛化能力
	* 一般来说，随着模型复杂度的提高，训练误差和测试误差都会先降低，但是再复杂会产生过拟合，导致训练误差小但是测试误差很大
* 模型选择的常用方法
	* 正则化regularization。在结构风险表达式上添加一个正则化项或罚项作为结构风险，使结构风险最小的模型
	* 交叉验证cross-validation。数据量大的时候可以分为训练集、验证集和测试集。数据量小的时候进行交叉验证：重复使用数据，分为训练集和测试集
		* 简单交叉验证：随机分为训练集和测试集，选择测试误差小的模型
		* S折交叉验证：随机分为多分，选不同的几分为训练集和测试集，选取平均测试误差小的模型
		* 留一交叉验证：每次只有一个数据作为测试数据，其他都是训练数据。
* 泛化能力。泛化误差：模型对未知数据的预测误差。泛化误差上界。 
* 生成模型与判别模型
	* 生成方法->生成模型
		* 主要学习联合概率分布P(X,Y)，利用P(X,Y)/P（X）来生成P(Y|X)。主要关心各种输入对应的各种可能输出
		* 典型模型：朴素贝叶斯、隐马尔可夫
		* 特点：可以还原出P(X,Y)，学习收敛速度快，可以更加逼近真实模型，可以应用于存在隐变量的模型
	* 判别方法->判别模型
		* 主要学习决策函数f(X)或者条件概率分布P(Y|X)。主要关心给定一个输入对应的特定输出
		* 典型模型：kNN、SVM、感知机、决策树、boost等等
		* 特点：无法还原出P(X,Y)，由于直接面向输入的预测，所以准确率更高，不考虑内部因素因而可以简化学习问题
* 分类问题
	* 输出是离散的预测问题。需要进行模型学习和分类预测
	* 常用评价指标准确率precision和召回率recall，以及F1值，2/F1 = 1/pre + 1/recall。
	* 常用方法：kNN、感知机、朴素贝叶斯、决策树、SVM等等
* 标注问题
	* 输入输出都是变量序列的问题。输入是一个观测序列，输出是一个标记序列或状态序列。
	* 常用方法：隐马尔可夫模型、条件随机场
	* 常用领域：信息抽取、NLP、词性标注等。如一句话是一个输入观测序列，输出标记序列是标注的词性序列。
* 回归问题
	* 输入和输出都是连续的变量的预测问题。
	* 回归问题主要就是进行函数拟合，根据函数的特性分为一元回归和多元回归，线性回归和非线性回归等。
	* 回归问题常用损失函数是平方损失函数，常用最小二乘法求解。

---
###  感知机 perceptron

* 感知机概念
	* 定义：将输入空间二分类到输出空间{+1，-1}的线性分类模型，函数表示为f(x)=sign(wx+b)，其中w是权值向量，b为偏置bias，wx是内积，sign是符号函数
	* 其模型对应分离超平面wx+b=0
* 感知机的学习策略
	* 损失函数：
		* 使用所有误分类点的数量不是连续可导不方便优化。
		* 使用所有误分类点到超平面的距离之和是连续可导的，感知机用损失函数为$L(w,b)=-sum_{x_i\in M}y_i(wx_i+b) $，其中M是误分类的点
	* 数据集的线性可分性。如果不是线性可分的，感知机训练会因为发散而漂浮不定。线性可分时，precetron被证明是一定会收敛。
* 感知机的学习算法
	* 原始形式求解
		* 思路：求解损失函数的极小值，可采用随机梯度下降stochastic gradient descent进行优化，然后随机选取误分类点进行w和b的更新
		* 算法步骤：选取初始w，b如0，遍历训练点，如果出现误分类（yi(w·xi+b)<0）则进行sgd调整w=w+lr·xi·yi，b=b+lr·yi，直至wb满足没有出现误分类点，其中a是学习率learning rate（0<lr<=1）
	* 对偶形式求解
		* 思路：w和b改写成为所有误分类点通过n次的lr 的表达式，训练数据只会以xi·xj的内积形式出现，可以进行预计算存储为Gram矩阵方便计算。
		* 算法步骤：初始化A、b为0，训练所有误分类点，出现误分类点(xi,yi)则进行sgd调整A=A+lr，b=b+yi，直至Ab满足没有误分类点。
* 特点
	* 适用于线性可分，二分类，属于判别模型，采用sgd求解，简单易实现，最终模型与初始参数、训练数据的迭代顺序有关。

---
### k近邻法 kNN

* kNN概念
	* 在训练集中查找新数据的k个最近邻数据，根据决策规则给新数据做出决策。kNN常用于分类和回归。
	* kNN实际上将特征空间划分为多个子空间cell单元，每一个cell对应一个决策结果。
* kNN模型
	* 三个基本要素：距离度量方法、k值选取、分类决策规则
	* 距离度量方法：Lp距离、Minkowski距离等。Lp距离是(sum|xi^-yi|^p)^(1/p)，当p=1就是曼哈顿距离（坐标差的绝对值之和），当p=2就是欧氏距离，当p=无穷大就是各个坐标距离的最大值。
	* k值的选取：k较小时容易过拟合、受噪声影响，k过大时模型简单但会产生较高预测误差。k=1就是最近邻算法。k一般较小，并用交叉验证选取。
	* 分类决策规则：常用多数表决的方式。
* kNN的实现算法kd-tree
	* 
















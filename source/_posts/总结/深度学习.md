## DeepLearning

## 数据集
### train/dev/test
* 训练集 train ：主要是用来训练模型的。
* 开发集 develop：主要用于模型参数调优。
* 验证集 validate：模型的最终优化和最终确定，比如选取效果最好的模型。
* 测试集 test：用于测试模型的泛化能力。


### batch、iteration、epoch、minibatch
* batch、iteration、epoch、minibatch
    * 不同的人叫法不同：每次SGD称为一个iteration(迭代，更新参数)，但是对于full training data和每次SGD使用的data不同的人称为：epoch/batch、batch/minibatch、epoch/minibatch。反正就是epoch指最大的，如果minibatch指最小的，batch视情况而定。
    * 拿epoch/batch举例：
        * epoch：训练集中的所有样本都训练完成一次称为一个epoch。
        * batch：在一次SGD中，使用训练集中的batchsize个样本，称为batch。
        * 举例：训练集有1000个样本，选取batchsize=10，那么训练完一次整个样本需要100次iteration，1epoch。
---
## 常见的网络基础结构

## layer
* conv2d：二维卷积操作，输入的两个维度和channel，输出的两个维度和channel。卷积优点在于：参数共享、窗口稀疏(filter之外的稀疏为0)等。
    * 卷积的核称为kernel、filter、patch.
* Pooling：将多个维度的数值进行合并，降维降噪，防止过拟合。常见`max_pooing`、`average_pooling`、`min_pooling`。
* Dropout：随机断开目标网络的连接，随机断开的机率是1-keep_prob。训练时进行dropout以防止训练过拟合，测试和预测时关闭dropout，因为模型已经定型了。



## loss
* `cross_entropy_loss` 交叉熵 ：适用于已知真实分布的情况下，评估预测的效果。比如已知图片训练集的分类，使用交叉熵计算预测的结果的熵。
* `nce_loss` 噪声对比估计损失函数(noise-contrastive estimation, NCE)
* `sampled_softmax_loss`

## Gradient Descent GD梯度下降

* [batch-GD,Mini-batch-GD,SGD,Online-GD 大数据背景下的梯度训练算法](http://www.cnblogs.com/richqian/p/4549590.html)
* 常见optimizer优化器：
    * Adam 快速，常用
    * Batch GD：计算loss函数在整个训练集上的梯度方向，沿着该方向进行迭代。缺点：大数据集上复杂度过高。
    * Mini-batch GD：每次选取训练集的子集进行训练。较为常用。
    * Stochastic GD：SGD随机梯度下降，每次只选一个数据进行训练。优点：速度比较快。缺点：收敛性不好，可能引起目标函数剧烈震荡，可能hit不到最优点。
    * Online GD：所有训练数据只使用1次，然后丢弃。优点：利用实时数据，可以得到模型的变化趋势。


|梯度下降算法|batch gd|mini-batch gd|stochastic gd|online gd|
|:---:   |:---:   |:---:   |:---:   |:---:   |
|训练集   |固定   |固定   |固定   |实时更新   |
|单次迭代样本数   |整个训练集   |训练集的子集   |单个样本   |根据具体算法定   |
|算法复杂度   |高   |一般   |低   |低   |
|时效性   |低   |一般(delta模型)   |一般(delta模型)   |高   |
|收敛性   |稳定   |稳定   |不稳定   |不稳定   |


* SGD 随机梯度下降，大数据集用不错。SGD改进，使用动量。
L-BFGS 在小数据集上不错，改进的Large-batch L-BFGS不错。
Conjugate Gradients 也在小数据集也不错。
Mini-batch GD 一般选20-1000大。

## activation

### 常见激活函数
* ReLU (Rectified Linear Unit) ：用于隐层神经元输出 `relu(x)=max(0,x)` 优点：计算简单；单侧抑制；兴奋边界更宽；稀疏激活性。
	* 优点：
	* 缺点：没有进行数据压缩，数据范围可能很大。很容易改变数据的分布。
    * 改进：
	    * Leaky ReLU：在小于0的部分不直接设置为0，而是斜率较小的曲线，如y=-0.01x。
	    * pReLU、random ReLU等，ReLU后面加上Batch Normalization优化数据分布。
* Sigmoid：用于隐层神经元输出 `sigmoid(x)=1/(1+e^(-x))` 缺点：饱和区导数趋近0，容易梯度消失；
* Softmax：用于多分类神经网络输出 `softmax(x)=e^x/sum_i(e^(x_i)))` 计算各个类别的可能性，进行概率归一化，概率高的为判断依据。只用于多于一个输出的神经元.
* tanh：`tanh(x)=(1-e^(-x))/(1+e^(-x))` 计算复杂。
* SeLU：用于隐层神经元输出 论文指出能保证分布为`均值为0，方差为1`，所以梯度更不易消失，支持更深网络，效果更好。

### 激活函数的 Normalization
* 1502.03167 Batch normalization论文指出，尽可能保证每一层的输入有相同的分布。因此在activation之后经常进行batch normalization(selu有相似功能，见snn论文)

* Batch normalization
* Layer normalization
* Weight normalization
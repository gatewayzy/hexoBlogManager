---
title: 2016 基于神经网络的词和文档语义向量表示方法研究
comments: true
date: 2016-8-8 21:36:10
updated: 2016-8-8 21:36:13
categories: AI Deep Learning
tags: 
- Word Embedding
---
**说明：**读博士论文《基于神经网络的词和文档语义向量表示方法研究》 来斯惟 中国科学院大学。
<!-- more -->

## 参考文章
***

## 基于神经网络的词和文档语义向量表示方法研究

* 来斯惟 中国科学院大学
* 2016.1 博士论文 [Blog here](http://licstar.net/archives/category/自然语言处理/)


### 目录

1. 绪论
2. 现有词的分布表示技术
3. 词向量表示技术的实验分析
4. 基于字词联合训练的中文表示及应用
1. 基于循环卷积网络的文档表示及应用
1. 总结与展望

---
### 绪论

**历史发展与相关技术**

* BOW：稀疏问题，丢失词序信息。维数灾难，复杂的上下文。
* 词向量：从广义上讲，传统的词袋子模型也是用向量描述文本，也应当被称作词的向量表示，但是这种向量是高维稀疏的。在本文中，“词向量”特指由神经网络模型得到的低维实数向量表示
* 分布假说：上下文相似的词，其语义也相似（词的分布表示的理论基础，各词向量模型均基于分布假说设计而成）
* 语义组合：一段话的语义由其各组成部分的语义以及它们之间的组合方法所确定（句子和文档的语义表示）


---
### 现有词的分布表示技术

#### 分布表示/词向量的归类

* Turian等人对分布表示的分类：
    * distributional representation
    * clusteringbased word representation
    * distributed representation

* 本文对分布表示的分类：
	* 基于矩阵的分布表示（distributional representation代表基于矩阵的分布表示，直译应该代表所有的分布表示）
	* 基于聚类的分布表示
	* 基于神经网络的分布表示（distributed representation、word embedding）

#### 分布表示

##### 基于矩阵的分布表示

* 基本理论方法
	* 构建一个“词-上下文”矩阵，每行对应一个词，每列对应上下文词，矩阵元素用二者共现次数。
* 具体步骤
	* 选取上下文：
		* 第一种，将词所在的文档作为上下文，形成“词-文档”矩阵；
		* 第二种，将词附近上下文中的各个词（如上下文窗口中的 5 个词）作为上下文，形成“词-词”矩阵；
		* 第三种，将词附近上下文各词组成的 n 元词组（n-gram）作为上下文。
		* 在这三种方法中，“词-文档”矩阵非常稀疏，而“词-词”矩阵相对较为稠密，效果一般好于前者。“词-n元词组”相对“词-词”矩阵保留了词序信息，建模更精确，但由于比前者更稀疏，实际效果不一定能超越前者
	* 确定矩阵中各元素的值。
		* 使用词与对应的上下文的共现次数。效果并不好。
		* 多种加权和平滑方法，最常用的有 tf-idf、 PMI 和直接取 log。
	* 矩阵分解（可选）。
		* “词-上下文”矩阵中高维且稀疏，使用降维技术可以减少噪声影响，损失部分信息。
		* 常用矩阵分解：奇异值分解（SVD）、非负矩阵分解（NMF）、典型关联分析（Canonical Correlation Analysis， CCA）、Hellinger PCA（HPCA）。
* 典型技术
	* LSA：经典的 LSA 就是使用“词-文档”矩阵， tf-idf 作为矩阵元素的值，并使用 SVD分解，得到词的低维向量表示。
	* GloVe：“词-词”矩阵，共现次数的对数为矩阵的值，矩阵分解借鉴推荐系统中基于隐因子分解（Latent Factor Model）的方法，在计算重构误差时，只考虑共现次数非零的矩阵元素，同时对矩阵中的行和列加入了偏移项等。

##### 基于聚类的分布表示

* 基本理论方法
	* 将词语进行多次聚类，可以用所属类别信息表示一个词。分类越相似的词语说明语义越相似。
* 典型技术
	* Brown clustering：布朗聚类是一种层级聚类方法，聚类结果为每个词的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。

##### 基于神经网络的分布表示

* 基本理论方法
	* 对目标词和上下文的关系进行建模，NN可以表示复杂的上下文和语义信息，而且可以通过组合方式将上下文语义进行组合，参数个数仅以线性增长，缓解维数灾难。
	* 构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。同时，神经网络语言模型对后期词向量的发展方向有着决定性的作用。
* 语言模型
	* 长度为m的字符串确定一个概率分布P(w1,w2,w3...)=p(w1)p(w2|w1)P(w3|w1,w2)...
	* 右侧太长会难以计算，常用n-gram的n元模型（右边每个条件概率之和自己之前的n-1个词有关），如一元模型P(w1,w2,w3...)=p(w1)p(w2)P(w3)...即此时各词相互独立，计算方便但是性能有限。
	* 常用的是二元bigram和三元trigram模型，能保留一定的词序信息。
* 典型技术
	* NNLM：神经网络语言模型。
	* LBL：log双线性语言模型
	* RNNLM：循环神经网络语言模型
	* C&W：第一个直接以生成词向量为目标的模型。没有采用语言模型，而是直接对n元短语打分，出现过的n元组合短语会打高分，未出现的随机短语打低分，仍符合分布假说。
	* CBOW和Skip-gram：Continuous Bag-of-Words。（tf提供Skip-gram实现）。CBOW：多个上下文词预测目标词，Skip-gram：目标词预测多组单个上下文词。
	* Order：
* 相关优化技术
	* 层级softmax
	* NCE：噪声对比估算，noise-contrastive estimation（tf的NCE_loss）
	* negative sampling：负采样技术
	* subsampling：二次采样技术
* 词向量模型的比较
	* 上下文选择：
		* Skip-gram使用词作为上下文，其他NN都是n-gram。但是构造n-gram的方法不同。
		* CBOW 模型使用 n-gram 中各词词向量的平均值作为上下文表示；
		* Order 模型使用 n-gram 中各词词向量的拼接作为上下文表示，这种方法可以看做词向量的线性组合； 
		* LBL 模型则是直接对 n-gram 中各词的词向量做了线性变换； 
		* NNLM 和 C&W 模型更是做了非线性变换，比如tanh、relu等非线性激活
	* 模型复杂度P21：Skip-gram>>CBOW>>Order>>LBL>>C&W、NNLM
	* 参数个数
	* 时间复杂度
	* 效率和性能
	* 目标词与上下文之间的关系：C&W是目标词与上下文二者联合打分，目标词也是输入。其他NN模型都是用上下文预测目标词。

#### 对比总结

|分布表示方法|上下文|上下文的表示|目标词与其上下文关系|上下文与目标词之间的建模方法|
|:---:|:---:|:---:|:---:|:---:|
|LSA/LSI	|文档	|	|	|矩阵	|
|HAL	|词	|	|	|矩阵	|
|GloVe	|词	|	|	|矩阵	|
|Jones & Mewhort	|n-gram	|	|	|矩阵	|
|Brown Clustering	|词		|	||聚类	|
|Skip-gram	|词	|上下文中某一个词的词向量	|上下文预测目标词	|NN	|
|CBOW	|n-gram（加权）	|上下文各词词向量的平均值	|上下文预测目标词	|NN	|
|Order	|n-gram（线性组合）	|上下文各词词向量的拼接	|上下文预测目标词	|NN	|
|LBL	|n-gram（线性组合）	|上下文各词的语义组合	|上下文预测目标词	|NN	|
|NNLM	|n-gram（非线性组合）	|上下文各词的语义组合	|上下文预测目标词	|NN	|
|C&W	|n-gram（非线性组合）	|上下文各词与目标词的语义组合	|上下文与目标词联合打分	|NN	|

* 这三类模型中，基于聚类的模型较为特别，将词表示为聚类的类标。如果采用层级聚类方法，可以根据聚类类别的公共前缀衡量词之间的相似度。而另外两类模型得到的都是向量表示，可以直接使用余弦距离、欧氏距离等向量空间距离衡量指标来检测词义的相似度。
* “词-词”矩阵做 SVD 分解与 Skip-gram 模型配合负采样技术优化具有相同的最优解（最优解可以取到的情况下），实际上很难取到最优解，差异较大。

### 词向量表示技术的实验分析

#### 模型选择与设置

1. 哪个模型效果更好？如何选取上下文、上下文与目标词的关系？
	* Skip-gram和CBOW构造的上下文方法导致其忽略了词序信息，LBL、NNML、C&W使用上下文窗口中各词词向量的拼接作为上下文的表示，可以保留词序信息。
	* 复杂的模型在大型预料中越具有优势，小规模的语料用简单的模型就可以获得足够好的效果。
	* C&W效果一般不太好，因为组合打分的方式往往使语义接近于同时出现的词语，比不上上下文预测目标词的方法。P39

2. 训练语料的大小及领域对词向量有什么样的影响？
	* 语料的领域比语料的规模更重要，领域确定后，语料规模越大越好。


3. 迭代多少次才能保证语义阻垢又不会过拟合？选取多少维词向量？
	* 迭代优化的终止条件最好根据具体任务的验证集来判断，或者近似地选取其它类似的任务作为指标，但是不应该选用训练词向量时的损失函数。
	* 词向量维度一般选择50维及以上，维度越大越好。

#### 实验与评价方法

1. 词向量的语言学特性
	* *利用词向量的语言学特性完成任务*
	* 语义相关性 (ws)：计算词向量余弦距离与数据集比较。
	* 同义词检测 (tfl)：计算问题词与选项词对应词向量之间的余弦距离，并选用距离最近的选项词，与数据集比较。
	* 单词类比 (sem、syn)：计算词向量的加减法与数据集中词语类比的结果进行比较。
2. 词向量用作特征
	* *将词向量作为特征，提高自然语言处理任务的性能*
	* 基于平均词向量的文本分类 (avg)：词向量的加权平均作为文本的特征，进行logistic回归完成文本分类，测试准确率。
	* 命名实体识别 (ner)：词向量作为ner系统的额外特征，在数据集测试准确率。
3. 词向量用作神经网络的初始值
	* *将词向量作为神经网络的初始值，提升神经网络模型的优化效果*
	* 基于卷积神经网络的文本分类 (cnn):词向量作为cnn进行文本分类任务的初始值，对数据集进行分类验证。
	* 词性标注 (pos):词向量作为pos标注网络初始值，对橘子中没歌词进行序列标注，测试准确率。